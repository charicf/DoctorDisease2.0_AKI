{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import platform\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Reader(object):\n",
    "    def __init__(self, dataset_dir, notes_dir=None, listfile=None):\n",
    "        self._dataset_dir = dataset_dir\n",
    "        self._notes_dir = notes_dir\n",
    "        self._current_index = 0\n",
    "        if listfile is None:\n",
    "            listfile_path = os.path.join(dataset_dir, \"listfile.csv\")\n",
    "        else:\n",
    "            listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        self._listfile_header = self._data[0]\n",
    "        self._data = self._data[1:]\n",
    "\n",
    "    def get_number_of_examples(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def random_shuffle(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(self._data)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        print(\"Reader read_example not implemented\")\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def read_note(self, nt_filename):\n",
    "        text = defaultdict(list)\n",
    "        doc_info = dict()\n",
    "        (id_patient, _) = nt_filename.split(\"_\")\n",
    "        with open(os.path.join(self._notes_dir, id_patient + \"/\" + nt_filename), \"r\") as ntfile:\n",
    "            tmp_sent = []\n",
    "            head = True\n",
    "            for line in ntfile:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if head:\n",
    "                        (id_doc, doc_cat, chart_date, chart_time, h, icu_id, h_id) = line.split(\",\")\n",
    "                        # read only the notes in the same episode\n",
    "                        if self.stay_id == icu_id:\n",
    "\n",
    "                            id_doc = int(id_doc)\n",
    "                            doc_info[id_doc] = (doc_cat, chart_date, chart_time, h, icu_id, h_id)\n",
    "                            head = False\n",
    "                        else:\n",
    "                            head = False\n",
    "                    else:\n",
    "                        if self.stay_id == icu_id:\n",
    "                            tmp_sent.append(line)\n",
    "                else:\n",
    "                    if self.stay_id == icu_id:\n",
    "                        text[id_doc].append(tmp_sent)\n",
    "                        tmp_sent = []\n",
    "                        head = True\n",
    "                    else:\n",
    "                        head = True\n",
    "\n",
    "        return (doc_info, text)\n",
    "\n",
    "    def read_next(self):\n",
    "        # print(\"Reader read_next\")\n",
    "        to_read_index = self._current_index\n",
    "        self._current_index += 1\n",
    "        if self._current_index == self.get_number_of_examples():\n",
    "            self._current_index = 0\n",
    "\n",
    "        return self.read_example(to_read_index)\n",
    "\n",
    "\n",
    "class InHospitalMortalityReader(Reader):\n",
    "    def __init__(self, dataset_dir, notes_dir=None, listfile=None, period_length=48.0):\n",
    "        \"\"\"Reader for in-hospital moratality prediction task.\n",
    "        :param dataset_dir:   Directory where timeseries files are stored.\n",
    "        :param listfile:      Path to a listfile. If this parameter is left `None` then\n",
    "                              `dataset_dir/listfile.csv` will be used.\n",
    "        :param period_length: Length of the period (in hours) from which the prediction is done.\n",
    "        \"\"\"\n",
    "        Reader.__init__(self, dataset_dir, notes_dir, listfile)\n",
    "        self._data = [line.split(\",\") for line in self._data]\n",
    "        self._data = [(n, x, int(y)) for (n, x, y) in self._data]\n",
    "        self._period_length = period_length\n",
    "        self.stay_id = None\n",
    "        # print(\"InHospitalMortalityReader init completed\")\n",
    "\n",
    "    def _read_timeseries(self, ts_filename):\n",
    "        ret = []\n",
    "        # print(\"path to read timeseries\", os.path.join(self._dataset_dir, ts_filename))\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(\",\")\n",
    "            # assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(\",\")\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        \"\"\"Reads the example with given index.\n",
    "        :param index: Index of the line of the listfile to read (counting starts from 0).\n",
    "        :return: Dictionary with the following keys:\n",
    "            X : np.array\n",
    "                2D array containing all events. Each row corresponds to a moment.\n",
    "                First column is the time and other columns correspond to different\n",
    "                variables.\n",
    "            t : float\n",
    "                Length of the data in hours. Note, in general, it is not equal to the\n",
    "                timestamp of last event.\n",
    "            text : text\n",
    "            y : int (0 or 1)\n",
    "                In-hospital mortality.\n",
    "            header : array of strings\n",
    "                Names of the columns. The ordering of the columns is always the same.\n",
    "            name: Name of the sample.\n",
    "        \"\"\"\n",
    "        if index < 0 or index >= len(self._data):\n",
    "            raise ValueError(\"Index must be from 0 (inclusive) to number of lines (exclusive).\")\n",
    "        note = self._data[index][0]\n",
    "        name = self._data[index][1]\n",
    "\n",
    "        # print(\"Reading example\", name)\n",
    "        # print(\"Reading note\", note)\n",
    "        # extract stay id from eg 70950_episode1_timeseries_214965.csv\n",
    "        # TODO add stay id into he time series file not in the name\n",
    "        file_name = os.path.splitext(name)[0]\n",
    "        (subject_id, episode_id, type_episode, stay_id) = file_name.split(\"_\")\n",
    "        self.stay_id = stay_id\n",
    "        t = self._period_length\n",
    "        y = self._data[index][2]\n",
    "        # TODO what to do with extra info now we read charted ordered sentences\n",
    "\n",
    "        # (doc_info, text) = self.read_note(note)\n",
    "        (X, header) = self._read_timeseries(name)\n",
    "\n",
    "        return {\n",
    "            \"X\": X,\n",
    "            \"t\": t,\n",
    "            # \"text\": text,\n",
    "            # \"text_info\": doc_info,\n",
    "            \"y\": y,\n",
    "            \"header\": header,\n",
    "            \"name\": name,\n",
    "        }\n",
    "\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timestep=0.8,\n",
    "        store_masks=True,\n",
    "        impute_strategy=\"zero\",\n",
    "        start_time=\"zero\",\n",
    "        config_path=os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"discretizer_config.json\"),\n",
    "    ):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config[\"id_to_channel\"]\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config[\"is_categorical_channel\"]\n",
    "            self._possible_values = config[\"possible_values\"]\n",
    "            self._normal_values = config[\"normal_values\"]\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i + 1] + eps\n",
    "\n",
    "        if self._start_time == \"relative\":\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == \"zero\":\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in [\"zero\", \"normal_value\", \"previous\", \"next\"]:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in [\"normal_value\", \"previous\"]:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == \"normal_value\":\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == \"previous\":\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == \"next\":\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins - 1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(\n",
    "                1.0 / (N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2)\n",
    "            )\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={\"means\": self._means, \"stds\": self._stds}, file=save_file, protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == \"2\":\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding=\"latin1\")\n",
    "            self._means = dct[\"means\"]\n",
    "            self._stds = dct[\"stds\"]\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret\n",
    "\n",
    "\n",
    "def read_chunk(reader, chunk_size):\n",
    "    # print(\"Reading chunk of size\", chunk_size)\n",
    "    data = {}\n",
    "    for i in range(chunk_size):\n",
    "        ret = reader.read_next()\n",
    "        # print(\"Ret:\", ret)\n",
    "        for k, v in ret.items():\n",
    "            # print(\"K:\",k,\"v:\", v)\n",
    "            if k not in data:\n",
    "                data[k] = []\n",
    "            data[k].append(v)\n",
    "    data[\"header\"] = data[\"header\"][0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bf310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
